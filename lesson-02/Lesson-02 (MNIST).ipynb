{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26402476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46acba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST image dataset, and then see what we have.\n",
    "\n",
    "# Note: This data is already divided up in such a way that it can be used for training and testing,\n",
    "# So there is no need to import train_test_split from scikit-learn that we used in lesson-01\n",
    "(train_images_, train_labels), (test_images_, test_labels) = datasets.mnist.load_data()\n",
    "\n",
    "# First we will 'normalize' our input data. \n",
    "# This may not always be necessary, but it is good practice, and makes finding the best model easier.\n",
    "# We'll set the numerical values to a range between 0 and 1\n",
    "train_images_, test_images_ = train_images_ / 255.0, test_images_ / 255.0  \n",
    "\n",
    "\n",
    "print(f\"train_images_.shape = {train_images_.shape}\")\n",
    "print(f\"train_labels.shape = {train_labels.shape}\")\n",
    "print(f\"train labels, first 20: {train_labels[0:20]}\")\n",
    "print()\n",
    "print(f\"test labels size: {len(test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1fde18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The images are 28 x 28 greyscale captures of handwritten digits, representing numbers from 0 to 9.\n",
    "\n",
    "# We've assigned 60,000 of the images for training, and 10,000 images for testing.\n",
    "# All these data are what we call Labeled Data: data for which we know the right answer.\n",
    "\n",
    "# Now lets take a look at the first one of them: This should be a '5' \n",
    "# according to the train labels output, above.\n",
    "img = train_images_[0]\n",
    "\n",
    "# We can show the grayscale image with PIL (aka pillow)\n",
    "# pil_image = Image.fromarray(img)\n",
    "# pil_image.show()\n",
    "\n",
    "# OR with matplotlip: This image shows grayscale brightness, \n",
    "# with the largest value reprenting the most dark grey.\n",
    "implot = plt.imshow(img)\n",
    "implot.set_cmap('Greys')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba13271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we show the same image with more colorful color map\n",
    "implot = plt.imshow(img)\n",
    "implot.set_cmap('YlGnBu')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f4397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import initializers\n",
    "\n",
    "# What model we want depends on many things, with the most important one being:\n",
    "# (1) how accurate are the predictions.\n",
    "#\n",
    "# Other important metrics to consider are:\n",
    "# (2) how big is the model, \n",
    "# (3) how long does it take to predict new data, and \n",
    "# (4) how long does it take to train.\n",
    "\n",
    "# Picking up where we left off, let's first try using a deep neural network\n",
    "\n",
    "# In lesson-01 we used only Linear or no activation.\n",
    "# For this model, we add relu to add some non-linear type behavior.\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Reshape((784,), input_shape=(28,28)))\n",
    "model.add(layers.Dense(200, kernel_initializer=initializers.RandomNormal(stddev=0.05), activation='relu'))\n",
    "model.add(layers.Dense(100, kernel_initializer=initializers.RandomNormal(stddev=0.05), activation='relu'))\n",
    "model.add(layers.Dense(50, kernel_initializer=initializers.RandomNormal(stddev=0.05), activation='relu'))\n",
    "\n",
    "# 10 neurons, one per output digit. Softmax is commonly used for the output in this categorical situation.\n",
    "model.add(layers.Dense(10, kernel_initializer=initializers.RandomNormal(stddev=0.05), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Note that there was no mention of the input data in this model creation.\n",
    "# The model is independing of the training data (other than the shape of the input).\n",
    "\n",
    "# Let's see how big the model is, and how many trainable parameters there are\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b56a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using only 10 epochs for starters and see how we did.\n",
    "# To keep the training process from using too much memory we'll send in batches of\n",
    "# 64 images, instead of all the images at once.\n",
    "import time\n",
    "start = time.time()\n",
    "model.fit(train_images_, train_labels, epochs=10, batch_size=64, validation_data=(test_images_, test_labels))\n",
    "end = time.time()\n",
    "print(f\"\\nTraining time is {round(end - start, 3)} seconds\\n\")\n",
    "      \n",
    "# Evaluate the model\n",
    "start = time.time()\n",
    "test_loss, test_acc = model.evaluate(test_images_, test_labels)\n",
    "end = time.time()\n",
    "print(\"\\nTest accuracy:\", round(test_acc * 100, 2))\n",
    "print(f\"\\nTesting time for 10,000 examples is {round(end - start, 3)} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e9b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this simple deep learning model we should get a test accuracy of 97.9 +/- 0.4 percent. \n",
    "# We also measured and reported the training time and the testing/prediction time.\n",
    "\n",
    "# Note that the training accuracy - which is reported as 'accuracy' while fitting our model to the training data - and \n",
    "# our test accuracy, reported as 'val_accuracy' - are typically a little different, but hopefully not too much. \n",
    "\n",
    "# In fact the val_accuracy - should have stopped getting better in any substantial\n",
    "# way after epoch 5 or 6, assuming this is the first time you ran it.\n",
    "\n",
    "# This may be a case of overfitting - and we can take some additional measure to imporove our model and reduce those effects.\n",
    "# These measures are called 'regularization', and I'll discuss them later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2719efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try adding a dropout layer\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "model = models.Sequential()\n",
    "dropout_fraction = 0.2\n",
    "\n",
    "model.add(tf.keras.layers.Reshape((784,), input_shape=(28,28)))\n",
    "model.add(tf.keras.layers.Dropout(dropout_fraction, input_shape=(784,)))\n",
    "model.add(layers.Dense(200, kernel_initializer=initializers.RandomNormal(stddev=0.05), activation='relu'))\n",
    "model.add(layers.Dense(100, kernel_initializer=initializers.RandomNormal(stddev=0.05), activation='relu'))\n",
    "model.add(layers.Dense(50, kernel_initializer=initializers.RandomNormal(stddev=0.05), activation='relu'))\n",
    "\n",
    "# 10 neurons, one per output digit. Softmax is commonly used for the output in this categorical situation.\n",
    "model.add(layers.Dense(10, kernel_initializer=initializers.RandomNormal(stddev=0.05), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Note that there was no mention of the input data in this model creation.\n",
    "# The model is independing of the training data (other than the shape of the input).\n",
    "\n",
    "# Let's see how big the model is, and how many trainable parameters there are\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32378258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will again, train the model using only 10 epochs for starters and see how we did.\n",
    "# To keep the training process from using too much memory we'll send in batches of\n",
    "# 64 images, instead of all the images at once.\n",
    "import time\n",
    "start = time.time()\n",
    "model.fit(train_images_, train_labels, epochs=10, batch_size=64, validation_data=(test_images_, test_labels))\n",
    "end = time.time()\n",
    "print(f\"\\nTraining time is {round(end - start, 3)} seconds\\n\")\n",
    "      \n",
    "# Evaluate the model\n",
    "start = time.time()\n",
    "test_loss, test_acc = model.evaluate(test_images_, test_labels)\n",
    "end = time.time()\n",
    "print(\"\\nTest accuracy:\", round(test_acc * 100, 2))\n",
    "print(f\"\\nTesting time for 10,000 examples is {round(end - start, 3)} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c52b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the dropout layer, which literally randomly ignores dropout_fraction (20%) of the data in each epoch had \n",
    "# the accuracy and val_accuracy staying much more in step with each other.\n",
    "\n",
    "# In all my test cases, the val_accuracy and training accuracy were more similar in the latter model with drop-out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93a797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also build and fit a new and different model...\n",
    "\n",
    "# Below we create a Convolutional Neural Network or CNN model in tensorflow/keras,\n",
    "# to train and predict these MNIST data.\n",
    "\n",
    "# We need to add one more dimension to the data to get this to feed into the models (and make the tensors flow).\n",
    "print(f\"Before reshaping, train_images_.shape = {train_images_.shape}\")\n",
    "\n",
    "# Add the extra dimension and create a new set of variables.\n",
    "train_images = np.reshape(train_images_, (60000, 28, 28, 1))\n",
    "test_images = np.reshape(test_images_, (10000, 28, 28, 1))\n",
    "\n",
    "print(f\"After reshaping, train_images.shape = {train_images.shape}\")\n",
    "\n",
    "# Recall that we already normalized our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301ea8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base model, Sequential, is basically a shell in which we can add layers including\n",
    "# convolutional layers, pooling steps, flattening, and dense neural newtork connections.\n",
    "# These various layers and their connections are specifying the network architecture.\n",
    "model = models.Sequential()\n",
    "\n",
    "# The type and size of the layers, how they change as we move toward the end of the sequence, \n",
    "# how big of a convolutional kernel (here we use 3x3), and what type of activation functions\n",
    "# are used (relu vs tanh vs sigmoid) on the inner layers are just some of the many meta-parameters\n",
    "# that one can exeriment with to find the best model for a given set of input data.\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "# 10 neurons, one per output digit. Softmax is commonly used for the output in this categorical situation.\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412fa7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that this CNN model has only 50% of the Trainable parameters as our deep neural network.\n",
    "\n",
    "# Train the model\n",
    "start = time.time()\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=64, validation_data=(test_images, test_labels))\n",
    "end = time.time()\n",
    "print(f\"\\nTraining time is {round(end - start, 3)} seconds\\n\")\n",
    "\n",
    "# Evaluate the model\n",
    "start = time.time()\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "end = time.time()\n",
    "print(\"\\nTest accuracy:\", round(test_acc * 100, 2))\n",
    "print(f\"\\nTesting time for 10,000 examples is {round(end - start, 3)} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465d3e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using this smaller model we obtained a higher test accuracy, although it did cost us by taking\n",
    "# longer to train, and longer to predict results. These are actually relatively small models.\n",
    "# Some production models can take many hours or even days to train.\n",
    "\n",
    "# So depending on your cost structure and how often you need to retrain and incorporate the latest data, there may\n",
    "# be a reason to trade off a little accuracy for faster data incorporation (feedback).\n",
    "\n",
    "# The CNN architecture - being more complex - requires a greater number of calculations\n",
    "# for each trainable parameter.\n",
    "\n",
    "# CNNs are designed to yield more accurate predictions and a better analysis of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f2311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should have resulted in about 99% accuracy. \n",
    "# It's good to ask whatever your result (99% or whatever): Is it good enough? ...and to dig a bit more into the details.\n",
    "\n",
    "# I alway like to look at a sample of the failed cases.\n",
    "# Are they really that bad or are we missing something in our model?\n",
    "\n",
    "# So how do we find out which ones were bad? \n",
    "# Let's start with getting the test case predictions.\n",
    "pred_labels_ = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b854fed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# During the training process, TensorFlow will internally convert numerical labels into the appropriate format depending on\n",
    "# the loss function you're using. In this particular case as one of 10 categorical labels.\n",
    "\n",
    "# So even though we feed it a 5 it automatically converts this to a vector, represented either as:\n",
    "[0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 1,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0]\n",
    "\n",
    "# or shown as: [0,0,0,0,0,1,0,0,0,0] \n",
    "\n",
    "# And so, our predicted labels are also in this format...\n",
    "\n",
    "print(f\"pred_labels_.shape = {pred_labels_.shape}\")\n",
    "print(f\"test_labels.shape = {test_labels.shape}\\n\")\n",
    "print(f\"pred_labels_[0] = {np.round(pred_labels_[0], 3)}\\ntest_labels[0] = {test_labels[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83edf8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we'll need to do some manipulations to make them more easily comparable.\n",
    "\n",
    "# Create a container of the same size as pred_labels, with only the single numerical value representing the best prediction.\n",
    "pred_labels = np.zeros((pred_labels_.shape[0]), dtype=np.int16)\n",
    "\n",
    "# Then fill it with the index with the max values.\n",
    "for i, p in enumerate(pred_labels_):\n",
    "    pred_labels[i] = np.argmax(p)\n",
    "\n",
    "# Now we can compare them and get an array indicating where there is agreement and where there is not.\n",
    "truth = (pred_labels == test_labels)\n",
    "\n",
    "failure_indexes = np.where(~truth)\n",
    "print(f\"Error rate = {sum(~truth)/len(truth)}\") \n",
    "\n",
    "print(f\"Indexes of failed predictions:\\n{failure_indexes}\")\n",
    "print()\n",
    "print(f\"Labels:      {test_labels[failure_indexes]}\")\n",
    "print(f\"Predictions: {pred_labels[failure_indexes]}\")\n",
    "\n",
    "# Looking at the first example where there was disagreement(failure).\n",
    "idx = failure_indexes[0][0]\n",
    "\n",
    "# Recall that we rescaled (normalized) the original train and test images, so we'll\n",
    "# have to undo that to see them in their original form. \n",
    "image_array = (test_images_[idx] * 255)\n",
    "\n",
    "implot = plt.imshow(image_array)\n",
    "implot.set_cmap('Greys')\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e793c048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, Can you see why the computer might get this one confused?\n",
    "# Look at the first value in the lists above to see what is predicted and what is the label.\n",
    "\n",
    "# Lets randomly sample one more from the remaining values\n",
    "idx_value = np.random.choice(failure_indexes[0][1:], size=None, replace=True) \n",
    "print(f\"Index Value = {idx_value}\")\n",
    "\n",
    "# Fix these next two lines\n",
    "print('Label:', test_labels[idx_value])\n",
    "print('prediction:', pred_labels[idx_value])\n",
    "\n",
    "image_array2 = (test_images_[idx_value] * 255)\n",
    "implot = plt.imshow(image_array2)\n",
    "implot.set_cmap('Greys')\n",
    "plt.colorbar()\n",
    "\n",
    "# Does it make sense that this one is challenging to predict?\n",
    "\n",
    "# After looking at many of these failure cases, often I can see where the confusion is coming from, very infrequently I \n",
    "# think the label is wrong, and sometimes I just can't figure out what I'm looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f81b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It would also be good to see if one type of number failed more than another, etc.\n",
    "plt.hist(test_labels[failure_indexes])\n",
    "plt.title(\"Distribution of Failed Handwriting Predictions\")\n",
    "plt.xlabel(\"Labeled Value\")\n",
    "plt.show()\n",
    "\n",
    "# If it looks like it is not doing a good job on an obvious number,\n",
    "# there are surely other tricks we can try to implement to bump up our accuracy.\n",
    "#  - Other architectures\n",
    "#  - More training\n",
    "#  - Different types of regularization\n",
    "#  etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85a6664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On to Lesson-03"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
