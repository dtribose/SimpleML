{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec06f613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets start with a simple fitting problem - finding the best fit line for a set of x,y points.\n",
    "# And we will compare the results using scipy and those obtained with machine learning, \n",
    "# and start noticing some of the differences\n",
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "m = 2\n",
    "b = -3\n",
    "y = [m*k + b for k in x]  # Calculate the y for a given x using our m,b.\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d579bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets add a little noise to the data\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import math\n",
    "import time\n",
    "\n",
    "time.time()\n",
    "random.seed(int(time.time()))\n",
    "\n",
    "mu, sigma = 0, 0.15\n",
    "\n",
    "# Noise that is normally distributed around 0.0 with sigma of 0.15\n",
    "dy = random.default_rng().normal(mu, sigma, size=len(y))\n",
    "y = y + dy\n",
    "y, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9066c38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first look at our data\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881e79cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll fit this using the scipy way - effectively it's a least squares fit\n",
    "from scipy import optimize\n",
    "\n",
    "def fnc(xx, m, b):\n",
    "    return m*xx + b\n",
    "\n",
    "res = optimize.curve_fit(fnc, x, y, p0=(1,1), full_output=False)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e0ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this tells us our slope is around 2, and our intercept is around -3 as expected.\n",
    "# Plotting these results, along with the orginal data, gives this\n",
    "\n",
    "plt.scatter(x,y)\n",
    "m0,b0 = res[0]   # Use the slope and the intercept that we calculated\n",
    "xout = [0, 5, 11]\n",
    "\n",
    "def f(ecks):\n",
    "    return fnc(ecks, m0, b0)\n",
    "\n",
    "yout = list(map(f, xout))\n",
    "plt.plot(xout, yout, linestyle='dashed', color='orange')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4882e362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets do this the \"ML way\". \n",
    "import tensorflow as tf\n",
    "\n",
    "(x_train, y_train) = np.array(x), np.array(y)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, input_shape=(1,))\n",
    "])\n",
    "\n",
    "# Use mean_squared_error as the loss, for consistency with the scipy way, \n",
    "# and using the default optimizer - Steepest Gradient Descent (SGD).\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "              loss='mean_squared_error',\n",
    "              metrics='accuracy')\n",
    "\n",
    "model.fit(x_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets plot these results\n",
    "y_pred = model.predict(xout)\n",
    "plt.scatter(x,y)\n",
    "plt.plot(xout, y_pred, linestyle='dashed', color='orange')\n",
    "plt.show()\n",
    "\n",
    "# Not quite as good as we'd like to see, but not random. Lets see if we can do better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ca67ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll allow it to train with more cycles, say 500\n",
    "model.fit(x_train, y_train, epochs=500)\n",
    "\n",
    "y_pred = model.predict(xout)\n",
    "plt.scatter(x,y)\n",
    "plt.plot(xout, y_pred, linestyle='dashed', color='orange')\n",
    "plt.show()\n",
    "\n",
    "# That should be much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0ce3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a one node neural network.\n",
    "# In general, each node multiplies your input by a matrix M and then adds a vector b.\n",
    "# y = Mx + b\n",
    "\n",
    "# Since we have only one point (per sample) we have a 1x1 matrix.\n",
    "# Can we introspect those weights.\n",
    "first_layer_weights = model.layers[0].get_weights()\n",
    "first_layer_weights\n",
    "\n",
    "# Looking at these results implies a slope of about 2 and a y-intercept of about -3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7082ae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note also that the accuracy that is shown is not changing, it always has a value of 0.0000e+00, i.e. 0.\n",
    "# This is because the 'accuracy' metric only applies to classification problems, and not to continuous variable results.\n",
    "\n",
    "# For the latter we can define a new metric using the keras.backend\n",
    "# rmse_accuracy as 100-rmse : root mean squared error (rmse) for regression (only for Keras tensors)\n",
    "def rmse_accuracy(y_true, y_pred):\n",
    "    from keras import backend\n",
    "    return (np.ones(backend.int_shape(y_pred)[0]) *  100) - backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "                                                                          \n",
    "# This will return 100% minus the RMSE errror as our regression analogy to accuracy,\n",
    "# which will make our results look more appealing and give us more confidence that things are working.\n",
    "                                                                              \n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=rmse_accuracy)                                     \n",
    "    \n",
    "model.fit(x_train, y_train, epochs=500)  # Again, using 500 epochs\n",
    "y_pred = model.predict(xout)\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.plot(xout, y_pred, linestyle='dashed', color='orange')\n",
    "plt.show()                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea7fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now see the accuracy increasing as the loss is decreasing.\n",
    "\n",
    "# Now lets try this with a slightly more complex model, and a few more data points\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "xx = [x for x in range(-15,16)]\n",
    "m = 2\n",
    "b = -3\n",
    "yy = [m*k + b for k in xx]\n",
    "\n",
    "XX = np.array(xx)\n",
    "\n",
    "print(f\"len(xx) = {len(xx)}\")\n",
    "\n",
    "(x_train, y_train) = XX, np.array(yy)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Add the first layer (input layer)\n",
    "# We set the input shape, which will be very useful when we have multidimensional input data like 2d images.\n",
    "model.add(layers.Dense(units=1, activation='linear', input_shape=(1,)))\n",
    "\n",
    "#Add another layer (hidden layer)\n",
    "model.add(layers.Dense(units=3, activation='linear'))\n",
    "\n",
    "# Add yet another layer (output layer)\n",
    "model.add(layers.Dense(units=1, activation=None))  # Gives a simple number.\n",
    "\n",
    "# Optionally, we can initalize the weights randomaly.\n",
    "# This can be helpful in avoiding a system where all zero initial values does not have any gradient. \n",
    "tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "\n",
    "# Use a mean_squared_error as the loss, and using the default optimizer, Steepest Gradient Descent.\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.0004),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=rmse_accuracy)\n",
    "\n",
    "# This will allow us to see how many parameters we have per layer, and the total number of trainable parameters.\n",
    "print(f\"model.summary = {model.summary()}\")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2cc465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a look at the weights...\n",
    "for i in range(3):\n",
    "    layer_i_weights = model.layers[i].get_weights()\n",
    "    print(f\"layer_{i}_weights = {layer_i_weights}\")\n",
    "    \n",
    "print('\\n')\n",
    "    \n",
    "# ...We will see that it is no longer obvious how our slope and intercept got embedded into these 12 parameters (see output).\n",
    "# In addition, the activation funtions were either None or linear: This is because if we chose the more tradition ones\n",
    "# like 'relu' or 'tanh' the results would have difficulty converging.\n",
    "\n",
    "# It was also necessary in this case to decrease the learning rate to make sure it converged. You can see this by\n",
    "# resetting the learning rate to 0.1, and see what happens. \n",
    "\n",
    "# We'll need to start thinking of normalizing our features and regulizing our training for stable results \n",
    "# in more general situations. I'll try to cover that in a later 'lesson'\n",
    "\n",
    "# Finally we need to think about how many trainable parameters we have relative to our total number of data points.\n",
    "# In this case we have 31 data points, and Trainable params of 12. \n",
    "# It's good to have # data points >> # of trainable params\n",
    "\n",
    "# To wrap up this training, lets plot these results\n",
    "plt.scatter(x_train, y_train)\n",
    "\n",
    "x_out = [-16, -7, 1, 7, 16]\n",
    "y_pred = model.predict(x_out)\n",
    "\n",
    "plt.plot(x_out, y_pred, linestyle='dashed', color='orange')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6816f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets solve the same problem using another Machine Learning framework\n",
    "\n",
    "# For example, lets try scikit learn - specifically scikit.models.LinearRegression\n",
    "import math\n",
    "import numpy as np  # note: was already imported\n",
    "from sklearn.model_selection import train_test_split  # This function is often used with other frameworks \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Prepare your data, using capital X as out input data name (as opposed to our original 'x')\n",
    "# Using y = mX + b, with m = 2.0, and b = -3.0\n",
    "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
    "\n",
    "# Sample target values\n",
    "y = [m*x + b for x in X.flatten()]\n",
    "y = y + dy  # dy, from before, a bit of noise.\n",
    "\n",
    "# The next step is not necessary, but almost always done.\n",
    "# If we know we have a line, we could get the best estimate of the fitted parameters m & b, by using all the points.\n",
    "# However, In general, we want to compare different models and see what works best, and we seldom know for sure\n",
    "# that we have a perfect linear relationship, so we need to set aside some portion of our data for testing.\n",
    "# Split your data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=3428)\n",
    "\n",
    "# Create the regression model.\n",
    "model = LinearRegression()\n",
    "\n",
    "# NOTE: no compile step in scikit-learn, as it does not include deep learning models. \n",
    "# Tensorflow often involves complicated architectures, and the compilation step allows for some graph optimization\n",
    "# to allow for a faster fitting process. This is not needed for the simpler models employed in scikit-learn.\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)  # Note, this fit() process does not show it's per-epoch progress.\n",
    "\n",
    "# Make predictions\n",
    "y_predX = model.predict(X)\n",
    "\n",
    "# Evaluate the fit and print results\n",
    "mse = mean_squared_error(X, y_predX)\n",
    "RMSE = math.sqrt(mse/len(y_predX))  \n",
    "r2 = r2_score(y, y_predX)\n",
    "\n",
    "print(\"Scikit-learn Regression results:\")\n",
    "print(f\"  Root Mean Squared Error: {round(RMSE, 6)}, R-squared: {round(r2, 6)}\")\n",
    "\n",
    "# Output fitting variables: m0, b0\n",
    "m0 = model.coef_\n",
    "b0 = model.intercept_\n",
    "print(f\"  slope:{np.round(m0, 3)}, intercept: {np.round(b0, 3)} <<== should be close to expectations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a4eaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: During fitting, this model does not have any output showing us it's progress toward a satisfactory model.\n",
    "# There are some discussions on the web & chatGPT of the existance of a verbose flag, but that did not work in the \n",
    "# above situation...\n",
    "\n",
    "# Finally, lets plot the raw data and results\n",
    "plt.scatter(np.ndarray.flatten(X),y)\n",
    "\n",
    "# Make predictions using our xout. \n",
    "# We need to reshape our xout to be two dimensional as scikit-learn typcally uses 2d input data , \n",
    "# as opposed to tensorflow which is anywhere from one to many dimensional.\n",
    "xout_skl = np.array(xout).reshape(3,1)  # 'skl' for scikit-learn.\n",
    "\n",
    "y_pred = model.predict(xout_skl)\n",
    "plt.plot(xout, y_pred, linestyle='dashed', color='orange')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc3d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving on to lesson_02"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
