{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d6f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets start with a simple fitting problem\n",
    "# That is fit a straight line using scipy and then with machine learning.\n",
    "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "m = 2\n",
    "b = -3\n",
    "y = [m*k + b for k in x]\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6d3cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a little noise to the data\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import math\n",
    "import time\n",
    "\n",
    "time.time()\n",
    "random.seed(int(time.time()))\n",
    "mu, sigma = 0, 0.15\n",
    "dy = random.default_rng().normal(mu, sigma, size=len(y))\n",
    "y = y + dy\n",
    "y, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ceb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first look at our data\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a20906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll fit this using the scipy way - effectively a least squares fit\n",
    "from scipy import optimize\n",
    "def fnc(xx, m, b):\n",
    "    return m*xx + b\n",
    "res = optimize.curve_fit(fnc, x, y, p0=(1,1), full_output=False)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193ffbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this tells us our slope is around 2, and our intercept is around -3 as expected.\n",
    "# Plotting these results, along with the orginal data, gives this\n",
    "\n",
    "plt.scatter(x,y)\n",
    "m0,b0 = res[0]   # Use the slope and the intercept that we calculated\n",
    "xout = [0, 5, 11]\n",
    "\n",
    "def f(ecks):\n",
    "    return fnc(ecks, m0, b0)\n",
    "\n",
    "yout = list(map(f, xout))\n",
    "plt.plot(xout, yout, linestyle='dashed', color='orange')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f437f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets do this the ML way. We'll use the 'mean_squared_error' for consistency\n",
    "import tensorflow as tf\n",
    "\n",
    "(x_train, y_train) = np.array(x), np.array(y)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, input_shape=(1,))\n",
    "])\n",
    "\n",
    "# Use a mean_squared_error as the loss, and using the default optimizer, Steepest Gradient Descent.\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "              loss='mean_squared_error',\n",
    "              metrics='accuracy')\n",
    "\n",
    "# We explicitly set metrics to None as there are no appropriate metrics embedded in Keras to use for this fit.\n",
    "# Most of them are categorical, which is for \n",
    "\n",
    "model.fit(x_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2c6006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets plot these results\n",
    "y_pred = model.predict(xout)\n",
    "plt.scatter(x,y)\n",
    "plt.plot(xout, y_pred, linestyle='dashed', color='orange')\n",
    "plt.show()\n",
    "\n",
    "# Not quite as good as we'd like to see, but not random. Lets see if we can do better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d594b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll allow it to train with more cycles, say 500\n",
    "model.fit(x_train, y_train, epochs=500)\n",
    "\n",
    "y_pred = model.predict(xout)\n",
    "plt.scatter(x,y)\n",
    "plt.plot(xout, y_pred, linestyle='dashed', color='orange')\n",
    "plt.show()\n",
    "\n",
    "# That should be much better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca1fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a one node neural network.\n",
    "# In general, each point should have a matrix M and then adds a vector b.\n",
    "# y = Mx + b\n",
    "\n",
    "# Since we have only one point we have a 1x1 matrix.\n",
    "# Can we introspect those weights.\n",
    "first_layer_weights = model.layers[0].get_weights()\n",
    "first_layer_weights\n",
    "\n",
    "# Looking at these results implies a slope of about 2 and a y-intercept of about -3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa4c27d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Note also that the accuracy that is shown is not changing, it always has a value of 0.0000e+00, i.e. 0.\n",
    "# This is because the 'accuracy' metric only applies to classification problems, and not to continuous variable results.\n",
    "\n",
    "# For the latter we can define a new metric using the keras.backend\n",
    "# rmse_accuracy as 100-rmse : root mean squared error (rmse) for regression (only for Keras tensors)\n",
    "def rmse_accuracy(y_true, y_pred):\n",
    "    from keras import backend\n",
    "    return (np.ones(backend.int_shape(y_pred)[0]) *  100) - backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "                                                                          \n",
    "# This will return 100% minus the RMSE errror, as our regression analogy to accuracy,\n",
    "# which will make our results look more appealing and give us more confidence that things are working.\n",
    "                                                                              \n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=rmse_accuracy)                                     \n",
    "    \n",
    "model.fit(x_train, y_train, epochs=500)  # Again, using 500 epochs\n",
    "y_pred = model.predict(xout)\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.plot(xout, y_pred, linestyle='dashed', color='orange')\n",
    "plt.show()                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6193a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now see the accuracy increasing as the loss is decreasing.\n",
    "\n",
    "# Try this with more data and a more complex model, and a few more points\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "xx = [x for x in range(-15,16)]\n",
    "m = 2\n",
    "b = -3\n",
    "yy = [m*k + b for k in xx]\n",
    "\n",
    "XX = np.array(xx)\n",
    "\n",
    "print(f\"len(xx) = {len(xx)}\")\n",
    "\n",
    "(x_train, y_train) = XX, np.array(yy)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Add the first layer (input layer)\n",
    "model.add(layers.Dense(units=1, activation='linear', input_shape=(1,)))\n",
    "\n",
    "##Add another layer (hidden layer)\n",
    "model.add(layers.Dense(units=3, activation='linear'))\n",
    "\n",
    "# Add yet another layer (output layer)\n",
    "model.add(layers.Dense(units=1, activation=None))  # Gives a simple number.\n",
    "\n",
    "tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "\n",
    "# Use a mean_squared_error as the loss, and using the default optimizer, Steepest Gradient Descent.\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.0004),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=rmse_accuracy)\n",
    "\n",
    "# We explicitly set metrics to None as there are no appropriate metrics embedded in Keras to use for this fit.\n",
    "# Most of them are categorical, which is for \n",
    "\n",
    "print(f\"model.summary = {model.summary()}\")\n",
    "\n",
    "model.fit(x_train, y_train, epochs=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab4ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a look at the weights...\n",
    "for i in range(3):\n",
    "    layer_i_weights = model.layers[i].get_weights()\n",
    "    print(f\"layer_{i}_weights = {layer_i_weights}\")\n",
    "    \n",
    "print('\\n')\n",
    "    \n",
    "# ...We see it is no longer obvious how our slope and intercept got embedded in these 12 parameters (see output).\n",
    "# In addition, the activation funtions were either None or linear, and this is because if we chose the more tradition ones\n",
    "# like 'relu' or 'tanh' the results would have difficulty converging. \n",
    "# It was also necessary in this case to decrease the learning rate to make sure it converged. You can see this by\n",
    "# resetting the learning rate to 0.1, and see what happens.\n",
    "\n",
    "# We'll need to start thinking of normalizing our features and regulizing our training for \n",
    "# stable results in more general situations\n",
    "# Finally we need to think about how many trainable parameters we have relative to our total data points.\n",
    "# In this case we have 31 data points, and Trainable params of 12. \n",
    "# It's good to have # data points >> # of trainable params\n",
    "\n",
    "# For completion, lets plot these results\n",
    "plt.scatter(x_train, y_train)\n",
    "\n",
    "x_out = [-14, -7, 1, 7, 16]\n",
    "y_pred = model.predict(x_out)\n",
    "\n",
    "plt.plot(x_out, y_pred, linestyle='dashed', color='orange')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8230fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets solve the same problem using another framework for ML\n",
    "\n",
    "# For example, lets try scikit learn - specifically scikit.models.LinearRegression\n",
    "import math\n",
    "import numpy as np  # note: was already imported\n",
    "from sklearn.model_selection import train_test_split  # Often used with other frameworks \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Prepare your data, using capital X as out input data (as opposed to our original 'x')\n",
    "# Using y = mX + b, with m = 2.0, and b = -3.0\n",
    "# X values are referred to as 'Features'\n",
    "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
    "\n",
    "# Sample target values\n",
    "y = [m*x + b for x in X.flatten()]\n",
    "y = y + dy  # dy, from before, a bit of noise.\n",
    "\n",
    "# This next step is not necessary, but almost always done.\n",
    "# If we know we have a line, we could get the best estimate of the fitted parameters m & b, by using all the points.\n",
    "# However, In genearal, we want to compare model and see what works best,\n",
    "#    and we seldom know for sure that we have a perfect linear relationship,\n",
    "#       so we need to set aside some portion of our data for testing.\n",
    "\n",
    "# Split your data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=2, random_state=3428)\n",
    "\n",
    "# Create the regression model.\n",
    "model = LinearRegression()\n",
    "\n",
    "# NOTE: no compile step here. \n",
    "# Tensorflow often involves complicated architectures, and the compilation step allows for some graph optimization\n",
    "# to allow for a faster fit step, which are not needed for the simpler models in scikit-learn.\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)  # not showing the fitting process.\n",
    "\n",
    "# Make predictions\n",
    "y_predX = model.predict(X)\n",
    "\n",
    "# Evaluate the fit and print results\n",
    "mse = mean_squared_error(X, y_predX)\n",
    "RMSE = math.sqrt(mse/len(y_predX))  \n",
    "r2 = r2_score(y, y_predX)\n",
    "\n",
    "print(\"Scikit-learn Regression results:\")\n",
    "print(f\"  Root Mean Squared Error: {round(RMSE, 6)}, R-squared: {round(r2, 6)}\")\n",
    "\n",
    "# Output fitting variables: m0, b0\n",
    "m0 = model.coef_\n",
    "b0 = model.intercept_\n",
    "print(f\"  slope:{np.round(m0, 3)}, intercept: {np.round(b0, 3)}, should be close to expectations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a0425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: During fitting, this model does not have any output to show us it's progress toward a satisfactory model.\n",
    "# There is discussed on the web & chatGPT of the existance of a verbose flag, but that did not work above. \n",
    "\n",
    "# plot raw data and results\n",
    "plt.scatter(np.ndarray.flatten(X),y)\n",
    "\n",
    "# Make predictions using our xout. \n",
    "# We need to reshape our xout to be two dimensional as scikit-learn typcally uses 2d input data , \n",
    "# as opposed to tensorflow which is anywhere from 1 to many dimensional \n",
    "xout_skl = np.array(xout).reshape(3,1)\n",
    "\n",
    "y_pred = model.predict(xout_skl)\n",
    "plt.plot(xout, y_pred, linestyle='dashed', color='orange')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0adee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving on to lesson_02"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
